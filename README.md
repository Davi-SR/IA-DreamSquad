# Case â€“ API de Chat com Agente de IA (FastAPI + Strands + Ollama)

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o de uma **API de Chat** simples utilizando **FastAPI** integrada a um **Agente de IA** com o **Strands Agents SDK**, utilizando a **Ollama** como LLM local.  
O agente Ã© capaz de:

- Responder perguntas de conhecimento geral.
- Identificar quando uma pergunta envolve **cÃ¡lculo matemÃ¡tico**.
- Utilizar uma **tool de cÃ¡lculo** (funÃ§Ã£o Python) para resolver operaÃ§Ãµes matemÃ¡ticas.

---

## ğŸ§© Objetivo do Case

Atender aos requisitos descritos no PDF do desafio:

- Criar uma API `POST /chat` com FastAPI.
- Integrar essa API a um agente de IA (Strands Agents).
- Configurar uma **Tool de CÃ¡lculo MatemÃ¡tico** para o agente.
- Rodar tudo localmente usando **Ollama** como modelo de linguagem.

---

## ğŸ—ï¸ Arquitetura Geral

VisÃ£o geral do fluxo:

1. O usuÃ¡rio envia uma requisiÃ§Ã£o `POST /chat` com um JSON:
   ```json
   { "message": "Quanto Ã© 2+2?" }
   ```
2. O FastAPI recebe a mensagem e chama a funÃ§Ã£o `run_agent(message)`.
3. `run_agent` encaminha a mensagem para um `Agent` do Strands:
   - O `Agent` usa o modelo configurado via `OllamaModel`.
   - Tem acesso a uma tool Python chamada `calcular`, usada para operaÃ§Ãµes matemÃ¡ticas.
4. O agente decide, com base na pergunta:
   - Se deve responder diretamente com conhecimento geral, ou
   - Se deve chamar a tool `calcular` para fazer contas.
5. A resposta final volta para o FastAPI e Ã© retornada ao cliente como:
   ```json
   { "response": "A resposta para a pergunta "Quanto Ã© 2+2?" Ã© 4." }
   ```

---

## ğŸ“ Estrutura do Projeto

Exemplo de estrutura:

```
.
â”œâ”€â”€ main.py          # API FastAPI (/chat)
â”œâ”€â”€ agent.py         # DefiniÃ§Ã£o do Agent, tools e integraÃ§Ã£o com Ollama
â”œâ”€â”€ requirements.txt # DependÃªncias do projeto
â”œâ”€â”€ .env             # VariÃ¡veis de ambiente (NÃƒO versionado)
â”œâ”€â”€ .gitignore       # Arquivos e pastas ignorados pelo Git
â””â”€â”€ README.md        # Este documento
```

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1. PrÃ©-requisitos

- **Python 3.10+** instalado
- **Ollama** instalado e rodando localmente  
  - Site oficial: [https://ollama.com/](https://ollama.com/)
  - ApÃ³s instalar, certifique-se de ter baixado um modelo, por exemplo:
    ```bash
    ollama pull llama3.1
    ```

### 2. Clonar o repositÃ³rio

```bash
git clone https://github.com/Davi-SR/IA-DreamSquad.git
cd IA-DreamSquad
```

### 3. (Opcional, mas recomendado) Criar ambiente virtual

Mesmo que vocÃª nÃ£o tenha usado, Ã© boa prÃ¡tica sugerir:

```bash
python -m venv venv
source venv/bin/activate      # Linux/macOS
venv\Scripts\ctivate         # Windows
```

### 4. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

O `requirements.txt` inclui, por exemplo:

- `fastapi`
- `uvicorn`
- `python-dotenv`
- `strands-agents` (com suporte a Ollama)
- outros pacotes necessÃ¡rios ao projeto

---

## ğŸ” VariÃ¡veis de Ambiente (`.env`)

Todas as configuraÃ§Ãµes de modelo e host da LLM sÃ£o feitas via `.env` (ou variÃ¡veis de ambiente).

Crie um arquivo `.env` na raiz do projeto com conteÃºdo semelhante a:

```env
# Modelo a ser usado no Ollama
LLM_MODEL=llama3.1

# URL do servidor Ollama
OLLAMA_BASE_URL=http://localhost:11434
```

- `LLM_MODEL` â†’ nome do modelo dentro do Ollama (ex.: `llama3.1`).
- `OLLAMA_BASE_URL` â†’ endereÃ§o do servidor do Ollama (por padrÃ£o, `http://localhost:11434`).

O carregamento dessas variÃ¡veis Ã© feito no `agent.py` com:

```python
from dotenv import load_dotenv
import os

load_dotenv()

LLM_MODEL = os.getenv("LLM_MODEL", "llama3.1")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
```

---

## ğŸ§  ImplementaÃ§Ã£o do Agente (Strands + Ollama)

Toda a lÃ³gica do agente estÃ¡ em `agent.py`.

### 1. Tool de CÃ¡lculo MatemÃ¡tico

```python
from strands import tool
import math

@tool
def calcular(expressao: str) -> str:
    """
    Avalia uma expressÃ£o matemÃ¡tica simples em Python e retorna o resultado.
    Exemplo: "1234 * 5678" ou "math.sqrt(144)"
    """
    contexto_seguro = {
        "math": math,
        "__builtins__": {}
    }

    try:
        resultado = eval(expressao, contexto_seguro, {})
        return str(resultado)
    except Exception as e:
        return f"Erro ao calcular: {e}"
```

- Decorador `@tool` â†’ registra a funÃ§Ã£o como **tool** que o agente pode chamar.
- Usa `eval` com um **contexto seguro** (`math` e sem `__builtins__`) para evitar execuÃ§Ã£o de cÃ³digo perigoso.
- Retorna o resultado como string.

### 2. ConfiguraÃ§Ã£o do modelo Ollama

```python
from strands.models.ollama import OllamaModel

ollama_model = OllamaModel(
    host=OLLAMA_BASE_URL,
    model_id=LLM_MODEL,
)
```

- `host` â†’ URL do servidor Ollama.
- `model_id` â†’ nome do modelo definido no `.env`.

Esse objeto Ã© o â€œconectorâ€ entre o Strands e o Ollama.

### 3. DefiniÃ§Ã£o do Agent

```python
from strands import Agent

agent = Agent(
    model=ollama_model,
    tools=[calcular],
    system_prompt=(
        "VocÃª Ã© um assistente de IA. "
        "Quando a pergunta envolver cÃ¡lculos matemÃ¡ticos ou operaÃ§Ãµes numÃ©ricas, "
        "use a ferramenta 'calcular' passando apenas a expressÃ£o matemÃ¡tica. "
        "Caso contrÃ¡rio, responda normalmente com seu conhecimento."
    ),
)
```

- `model=ollama_model` â†’ define qual LLM o agente usa.
- `tools=[calcular]` â†’ ferramentas disponÃ­veis para o agente.
- `system_prompt` â†’ instruÃ§Ãµes de comportamento:
  - orienta o uso da tool de cÃ¡lculo,
  - orienta respostas normais para perguntas de conhecimento geral.

### 4. FunÃ§Ã£o `run_agent`

```python
async def run_agent(message: str) -> str:
    """
    Recebe a mensagem do usuÃ¡rio, envia para o agente e retorna APENAS o texto principal da resposta.

    EstratÃ©gia:
    1. Chama o agente com a mensagem.
    2. Tenta extrair response["content"][0]["text"], que Ã© o formato mais comum.
    3. Se nÃ£o conseguir, devolve a resposta convertida para string.
    """
    # 1. Envia a mensagem para o agente e espera a resposta
    response = await agent.invoke_async(message)

    # 2. Se a resposta for um dicionÃ¡rio no formato:
    #    {"role": "assistant", "content": [{"text": "alguma coisa"}]}
    if isinstance(response, dict):
        content = response.get("content")
        if isinstance(content, list) and content:
            first_item = content[0]
            if isinstance(first_item, dict) and "text" in first_item:
                return str(first_item["text"])

    # 3. Se a resposta jÃ¡ for string, sÃ³ devolve
    if isinstance(response, str):
        return response

    # 4. Se tiver atributo .content com lista semelhante
    if hasattr(response, "content"):
        c = response.content
        if isinstance(c, list) and c:
            first_item = c[0]
            if isinstance(first_item, dict) and "text" in first_item:
                return str(first_item["text"])
        return str(c)

    # 5. Fallback: devolve qualquer coisa como string
    return str(response)
```

**Resumo conceitual**:

- `invoke_async(message)` â†’ chama o agente (Strands + Ollama + tools).
- A funÃ§Ã£o tenta â€œdesembrulharâ€ a resposta para pegar apenas o texto (`"text"`) retornado pelo agente.
- Sempre retorna uma `str`, que serÃ¡ enviada ao cliente pela API.

---

## ğŸŒ ImplementaÃ§Ã£o da API (FastAPI)

Toda a API estÃ¡ em `main.py`.

### 1. Modelos de Entrada/SaÃ­da

```python
from fastapi import FastAPI
from pydantic import BaseModel
from agent import run_agent

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str

app = FastAPI()
```

- `ChatRequest` â†’ define o formato do JSON de entrada:
  ```json
  { "message": "texto do usuÃ¡rio" }
  ```
- `ChatResponse` â†’ define o formato do JSON de saÃ­da:
  ```json
  { "response": "resposta do agente" }
  ```

### 2. Endpoint `/chat`

```python
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(body: ChatRequest):
    user_message = body.message
    resposta_do_agente = await run_agent(user_message)
    return ChatResponse(response=resposta_do_agente)
```

Fluxo:

1. Recebe `POST /chat` com um corpo JSON `{"message": "..."}`.
2. Extrai `body.message` para `user_message`.
3. Chama `await run_agent(user_message)` para obter a resposta da IA.
4. Retorna um `ChatResponse` com o campo `response`.

---

## â–¶ï¸ Como Executar o Projeto

1. Certifique-se de que o **Ollama** estÃ¡ instalado e rodando:
   ```bash
   ollama serve
   ```
   E que o modelo configurado no `.env` (ex.: `llama3.1`) jÃ¡ foi baixado:
   ```bash
   ollama pull llama3.1
   ```

2. Instale as dependÃªncias:

   ```bash
   pip install -r requirements.txt
   ```

3. Execute o servidor FastAPI com Uvicorn:

   ```bash
   uvicorn main:app --reload
   ```

4. Acesse a documentaÃ§Ã£o interativa (Swagger UI):

   - http://127.0.0.1:8000/docs

5. Teste o endpoint `/chat`:

   - Exemplo de entrada (no `/docs` â†’ `POST /chat`):

     ```json
     {
       "message": "Quanto Ã© 2+2?"
     }
     ```

   - Exemplo de saÃ­da:

     ```json
     {
       "response": "A resposta para a pergunta "Quanto Ã© 2+2?" Ã© 4."
     }
     ```

   - Pergunta de conhecimento geral:

     ```json
     {
       "message": "Quem foi Albert Einstein?"
     }
     ```

---

## âœ… Requisitos Atendidos (segundo o PDF)

1. **ConfiguraÃ§Ã£o do Ambiente**
   - `.env` com configuraÃ§Ãµes do modelo e host do LLM.
   - `requirements.txt` com dependÃªncias (FastAPI, Strands, python-dotenv, etc.).

2. **ImplementaÃ§Ã£o da API (FastAPI)**
   - Endpoint `POST /chat` que recebe `{"message": "..."}` e retorna `{"response": "..."}`.
   - ExecutÃ¡vel via `uvicorn main:app --reload`.

3. **ImplementaÃ§Ã£o do Agente (Strands Agents)**
   - Agente configurado com `OllamaModel`.
   - Tool de cÃ¡lculo matemÃ¡tico (`@tool calcular`).
   - Capaz de responder:
     - Perguntas matemÃ¡ticas (ex.: `"Quanto Ã© 1234 * 5678?"`, `"Qual a raiz quadrada de 144?"`).
     - Perguntas de conhecimento geral (ex.: `"Quem foi Albert Einstein?"`).

4. **Versionamento**
   - `.gitignore` configurado (incluindo `.env` e arquivos de cache/log).

---

## ğŸ’¡ ObservaÃ§Ãµes Pessoais (Opcional, mas recomendÃ¡vel)

> Antes deste case, eu nÃ£o tinha trabalhado com Strands Agents nem com Ollama.  
> Durante a implementaÃ§Ã£o, aprendi:
>
> - Como estruturar uma API com FastAPI e Pydantic.
> - Como configurar e consumir um modelo local via Ollama.
> - Como criar um agente com tools usando o Strands Agents SDK.
> - Como trabalhar com variÃ¡veis de ambiente usando `python-dotenv`.
>
> TambÃ©m enfrentei alguns desafios com:
>
> - DiferenÃ§as entre versÃµes da biblioteca Strands (mÃ©todos e formatos de resposta).
> - Formato da resposta do agente (estrutura com `role`, `content`, `text`).
>
> Para este case, optei por uma lÃ³gica de pÃ³s-processamento simples em `run_agent`,
> priorizando clareza do cÃ³digo e entendimento da arquitetura completa.

---

## ğŸ“š ReferÃªncias

- **FastAPI â€“ DocumentaÃ§Ã£o Oficial**  
  https://fastapi.tiangolo.com/
- **Strands Agents SDK â€“ DocumentaÃ§Ã£o**  
  https://strandsagents.com/latest/documentation/docs/
- **Ollama â€“ Modelos locais**  
  https://ollama.com/
- **python-dotenv â€“ Gerenciamento de VariÃ¡veis de Ambiente**  
  https://pypi.org/project/python-dotenv/
